doInstall <- TRUE  # Change to FALSE if you don't want packages installed.
toInstall <- c("ROAuth", "streamR", "Rfacebook", "stringr", "httr", "devtools",
"maps", "igraph", "ggplot2", "quanteda", "reshape2", "scales")
if (doInstall) install.packages(toInstall)
if (doInstall) devtools::install_github("netdem-USC/netdemR")
library(ROAuth)
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
consumerKey <- "k4Pc1DC6ld3jnJhgrKy4tvlMV"
consumerSecret <- "k2v6JnAdQS9BEO9UCXDPYpj9JfyQZEMMlCt3o9gATxXghKQKTZ"
my_oauth <- OAuthFactory$new(consumerKey=consumerKey,
consumerSecret=consumerSecret, requestURL=requestURL,
accessURL=accessURL, authURL=authURL)
my_oauth <- OAuthFactory$new(consumerKey=consumerKey,
consumerSecret=consumerSecret, requestURL=requestURL,
accessURL=accessURL, authURL=authURL)
my_oauth$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
save(my_oauth, file="../credentials/twitter-token.Rdata")
setwd("~/git/social-media-workshop/code")
save(my_oauth, file="../credentials/twitter-token.Rdata")
library(streamR)
load("../credentials/twitter-token.Rdata")
filterStream(file.name="trump-tweets.json", track="trump",
timeout=20, oauth=my_oauth)
tweets <- parseTweets("trump-tweets.json")
tweets[1,]
filterStream(file.name="politics-tweets.json",
track=c("graham", "sessions", "trump", "clinton"),
tweets=20, oauth=my_oauth)
filterStream(file.name="tweets_geo.json", locations=c(-125, 25, -66, 50),
timeout=30, oauth=my_oauth)
library(ndjson)
install.packages("ndjson")
library(ndjson)
json <- stream_in("tweets_geo.json")
nrow(json)
str(json)
str(json)
json
library(netdemR)
library(streamR)
searchTweets(q=c("doug jones", "roy moore"),
filename="senator-tweets.json",
n=1000, until="2017-12-15",
oauth_folder="../credentials")
tweets <- parseTweets("senator-tweets.json")
sample(tweets$text, 1)
library(stringr)
ht <- str_extract_all(tweets$text, "#(\\d|\\w)+")
ht <- unlist(ht)
head(sort(table(ht), decreasing = TRUE))
followers <- getFollowers("BJPolS",
oauth_folder="../credentials")
friends <- getFriends("BJPolS",
oauth_folder="../credentials")
# extract profile descriptions
users <- getUsersBatch(ids=friends, oauth_folder="../credentials")
library(quanteda)
tw <- corpus(users$description[users$description!=""])
dfm <- dfm(tw, remove=c(stopwords("english"), stopwords("spanish"),
"t.co", "https", "rt", "rts", "http"),
remove_punct=TRUE)
# create wordcloud
par(mar=c(0,0,0,0))
textplot_wordcloud(dfm, rot.per=0, scale=c(3, .50), max.words=100)
users <- searchUsers(q="cologne political science", count=100, oauth_folder="../credentials")
users <- searchUsers(q="cologne political science", count=100, oauth_folder="../credentials")
users <- searchUsers(q="political science", count=100, oauth_folder="../credentials")
str(users)
users$screen_name[1:10]
getStatuses(ids=c("474134260149157888", "266038556504494082"), filename="old-tweets.json",
oauth_folder="../credentials")
parseTweets("old-tweets.json")
fb_oauth = 'EAACEdEose0cBAKsf0F6y6MN1MyFUVD1bOQasjzwY046ZAWPQsQ359xlG64jX3LcveTNjm68EOSZAHrLh3EiAODnwZC6lEKnNnDyHAyGYddplhabyZA6K7JO9Glys6gN45NnYySYO4lGy5bJhbdW01jx3FxRAO8udNzoNqOidp2U9TkwteXAGuEG2I6q22q0ZD'
library(Rfacebook)
fb_oauth = 'EAACEdEose0cBAKsf0F6y6MN1MyFUVD1bOQasjzwY046ZAWPQsQ359xlG64jX3LcveTNjm68EOSZAHrLh3EiAODnwZC6lEKnNnDyHAyGYddplhabyZA6K7JO9Glys6gN45NnYySYO4lGy5bJhbdW01jx3FxRAO8udNzoNqOidp2U9TkwteXAGuEG2I6q22q0ZD'
getUsers("me", token=fb_oauth, private_info=TRUE)
page <- getPage("DonaldTrump", token=fb_oauth, n=20, reactions=TRUE, api="v2.9")
page[1,]
page[which.max(page$likes_count),]
page <- getPage("104544669596569", token=fb_oauth, n=100, reactions=TRUE, api="v2.9")
page[which.max(page$likes_count),]
page[which.max(page$comments_count),]
page[which.max(page$shares_count),]
page <- getPage("barackobama", token=fb_oauth, n=100,
since='2012/11/01', until='2012/11/10')
page[which.max(page$likes_count),]
post_id <- page$id[which.max(page$likes_count)]
post <- getPost(post_id, token=fb_oauth, n.comments=1000, likes=FALSE)
comments <- post$comments
head(comments)
comments[which.max(comments$likes_count),]
group <- getGroup("150048245063649", token=fb_oauth, n=50)
str(group)
?getGroup
library(quanteda)
tweets <- read.csv('../data/candidate-tweets.csv', stringsAsFactors=F)
# loading lexicon of positive and negative words (from Neal Caren)
lexicon <- read.csv("../data/lexicon.csv", stringsAsFactors=F)
pos.words <- lexicon$word[lexicon$polarity=="positive"]
neg.words <- lexicon$word[lexicon$polarity=="negative"]
# a look at a random sample of positive and negative words
sample(pos.words, 10)
sample(neg.words, 10)
